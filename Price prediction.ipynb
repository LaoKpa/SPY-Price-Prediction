{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/alldat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'data exploration.ipynb',\n",
       " 'datasets',\n",
       " 'Price prediction.ipynb',\n",
       " 'Untitled.ipynb']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97890, 42)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat = np.array(df)[:,1:]\n",
    "dat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.248500e+02 1.162400e+02 6.262000e+01 7.991200e+02 1.157300e+02\n",
      " 7.767800e+02 3.656500e+01 8.559000e+01 5.402000e+01 1.169545e+03\n",
      " 2.255000e+01 1.765400e+02 8.720000e+01 9.079000e+01 2.954000e+01\n",
      " 1.618000e+02 7.890000e+01 1.141900e+02 5.581000e+01 1.184000e+02\n",
      " 1.352000e+02 2.544000e+03 1.390000e+03 4.120000e+02 3.000000e+02\n",
      " 1.479000e+04 1.000000e+02 1.000000e+00 0.000000e+00 1.300000e+03\n",
      " 0.000000e+00 2.946300e+04 0.000000e+00 3.150000e+03 1.000000e+02\n",
      " 0.000000e+00 4.300000e+01 1.000000e+02 0.000000e+00 2.510000e+03\n",
      " 2.000000e+00 4.000000e+02]\n"
     ]
    }
   ],
   "source": [
    "print(dat[0,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390\n",
      "251\n"
     ]
    }
   ],
   "source": [
    "nMin = 390\n",
    "nDay = int(dat.shape[0]/nMin)\n",
    "print(nMin)\n",
    "print(nDay)\n",
    "\n",
    "window = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createCNNFeatureVectors(dat, window):\n",
    "    vec_list = []\n",
    "    for day in range(nDay):\n",
    "        base = day * nMin\n",
    "        for min_ind in range(window, nMin - 1):\n",
    "            vec = np.zeros((2, 22, window))\n",
    "            vec[0,:21,:] = dat[(base+min_ind-window):(base+min_ind), :21].T\n",
    "            vec[0, 21,:] = dat[(base+min_ind-window):(base+min_ind), 0].T%1\n",
    "            vec[1,:21,:] = dat[(base+min_ind-window):(base+min_ind), 21:].T\n",
    "            vec_list.append(vec)\n",
    "    return torch.from_numpy(np.array(vec_list)).float()\n",
    "\n",
    "def createClassLabels(dat, window, yCol):\n",
    "    labels_per_day = nMin - window - 1\n",
    "    labels = np.zeros(nDay * labels_per_day)\n",
    "    for day in range(nDay):\n",
    "        base = day * nMin\n",
    "        labels[day * labels_per_day : (day + 1) * labels_per_day] = (dat[(base + 1):(base + 1 + labels_per_day), yCol] > dat[(base):(base + labels_per_day), yCol])\n",
    "    \n",
    "    return torch.from_numpy(labels).int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92619"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dat = createCNNFeatureVectors(dat, 20)\n",
    "len(X_dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([92619])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_col = 0\n",
    "Y_dat = createClassLabels(dat, window, pred_col)\n",
    "Y_dat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([92619])\n",
      "torch.Size([92619, 2, 22, 20])\n"
     ]
    }
   ],
   "source": [
    "print(Y_dat.shape)\n",
    "print(X_dat.shape)\n",
    "batch_size = 10\n",
    "\n",
    "train_X = X_dat[0:int(.8*len(X_dat))]\n",
    "train_Y = Y_dat[0:int(.8*len(X_dat))]\n",
    "\n",
    "train = torch.utils.data.TensorDataset(X_dat, Y_dat)\n",
    "\n",
    "val_X = X_dat[int(.8*len(X_dat)):]\n",
    "val_Y = Y_dat[int(.8*len(X_dat)):]\n",
    "\n",
    "train = torch.utils.data.TensorDataset(train_X, train_Y)\n",
    "val = torch.utils.data.TensorDataset(val_X, val_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class priceNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(priceNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 2, [3,3], stride = 1, padding = 1, bias = False)\n",
    "        #self.batch1 = nn.BatchNorm2d(4)\n",
    "        self.relu = nn.Tanh()\n",
    "        self.conv2 = nn.Conv2d(2, 2, [3,3], stride = 1, padding = 0, bias = True)\n",
    "        self.conv3 = nn.Conv2d(2, 2, [3,3], stride = 1, padding = 1, bias = True)\n",
    "        self.conv4 = nn.Conv2d(2, 1, [3,3], stride = 1, padding = 1, bias = True)\n",
    "        self.pool1 = nn.MaxPool2d([2,2])\n",
    "        self.pool2 = nn.MaxPool2d([2,2])\n",
    "        self.lin = nn.Linear(16, 1)\n",
    "        self.drop = nn.Dropout2d(p=0.2)\n",
    "        '''\n",
    "        self.conv1.weight = nn.Parameter(torch.rand((4,1,3,3))-.5)\n",
    "        self.batch1.weight = nn.Parameter(torch.rand((4,))-.5)\n",
    "        self.batch1.bias = nn.Parameter(torch.rand((4,))-.5)\n",
    "\n",
    "        self.conv2.weight = nn.Parameter(torch.rand((4,4,3,3))-.5)\n",
    "        self.conv2.bias = nn.Parameter(torch.rand(4,)-.5)\n",
    "\n",
    "        self.conv3.weight = nn.Parameter(torch.rand((4,4,3,3))-.5)\n",
    "        self.conv3.bias = nn.Parameter(torch.rand(4,)-.5)\n",
    "\n",
    "        self.lin.weight = nn.Parameter(torch.rand((36,64*64))-.5)\n",
    "        '''\n",
    "        '''\n",
    "        for param in self.parameters():\n",
    "            param.data = param.data.half()\n",
    "        '''\n",
    "    def forward(self, x):\n",
    "        #print(x.element_size() * x.nelement())\n",
    "        #print(x.shape)\n",
    "        f1 = self.conv2(x.view((x.shape[0], 2, 21, -1)))\n",
    "        #print(f1.shape)\n",
    "        #f2 = self.conv2(self.relu(f1))\n",
    "        #print(f2.shape)\n",
    "        f3 = self.pool1(self.drop(f1))\n",
    "        #print(f3.shape)\n",
    "        f4 = self.pool2(self.conv3(self.drop(self.relu(f3))))\n",
    "        #print(f4.shape)\n",
    "        f5 = self.conv4(self.drop(self.relu(f4)))\n",
    "        #print(f5.shape)\n",
    "        f6 = self.lin(f5.view((x.shape[0], 16)))\n",
    "        #print(f6.shape)\n",
    "        #f7 = self.soft(f6)\n",
    "        return f6.view(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fit_and_validate(net, loss_func, optimizer, train, val, n_epochs, batch_size =100):\n",
    "    \"\"\"\n",
    "    @param net: the neural network\n",
    "    @param optimizer: a optim.Optimizer used for some variant of stochastic gradient descent\n",
    "    @param train: a torch.utils.data.Dataset\n",
    "    @param val: a torch.utils.data.Dataset\n",
    "    @param n_epochs: the number of epochs over which to do gradient descent\n",
    "    @param batch_size: the number of samples to use in each batch of gradient descent\n",
    "    @return train_epoch_loss, validation_epoch_loss: two arrays of length n_epochs+1, containing the mean loss at the beginning of training and after each epoch\n",
    "    \"\"\"\n",
    "    net.eval() #put the net in evaluation mode\n",
    "    train_dl = torch.utils.data.DataLoader(train, batch_size)\n",
    "    val_dl = torch.utils.data.DataLoader(val, batch_size)\n",
    "    with torch.no_grad():\n",
    "        # compute the mean loss on the training set at the beginning of iteration\n",
    "        total=0\n",
    "        b = 0\n",
    "        t_l = 0\n",
    "        for X,Y in train_dl:\n",
    "            b+=1\n",
    "            X_temp = X.float().cuda()\n",
    "            Y_temp = Y.float().cuda()\n",
    "            pred = net(X_temp)\n",
    "            t_l += loss_func(pred, Y_temp)\n",
    "            total += Y.shape[0]\n",
    "\n",
    "            del X_temp\n",
    "            del Y_temp\n",
    "            \n",
    "        print(\"avg initial loss:\", t_l/total)\n",
    "\n",
    "    for i in range(n_epochs):\n",
    "        print(torch.cuda.memory_allocated(0))\n",
    "        print(\"base epoch #\", i)\n",
    "        net.train() #put the net in train mode\n",
    "        first = True\n",
    "        train_epoch_loss = []\n",
    "        val_epoch_loss = []\n",
    "        first = True\n",
    "        batch = 0\n",
    "        for X,Y in train_dl:\n",
    "            X_temp = X.float().cuda()\n",
    "            pred = net(X_temp)\n",
    "            #print(pred)\n",
    "            Y_temp = Y.float().cuda()\n",
    "            \n",
    "            loss = loss_func(pred, Y_temp)\n",
    "            if batch%1000 == 0:\n",
    "                print(\"batchnum: \", batch)\n",
    "                print(pred)\n",
    "                print(Y_temp)\n",
    "                print(loss)\n",
    "                \n",
    "            batch+=1\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            del Y_temp\n",
    "            del X_temp\n",
    "        with torch.no_grad():\n",
    "            net.eval() #put the net in evaluation mode\n",
    "            t_l = 0\n",
    "            v_l = 0\n",
    "            total=0\n",
    "            b = 0\n",
    "            for X,Y in train_dl:\n",
    "                b+=1\n",
    "                X_temp = X.float().cuda()\n",
    "                Y_temp = Y.float().cuda()\n",
    "                pred = net(X_temp)\n",
    "                t_l += loss_func(pred, Y_temp)\n",
    "\n",
    "                total += Y.shape[0]\n",
    "\n",
    "                del X_temp\n",
    "                del Y_temp\n",
    "\n",
    "                break\n",
    "            t_l/=total\n",
    "            #print(\"bias:\", sum(pred))\n",
    "            total=0\n",
    "            b=0\n",
    "            for X,Y in val_dl:\n",
    "                b+=1\n",
    "                X_temp = X.float().cuda()\n",
    "                Y_temp = Y.float().cuda()\n",
    "                pred = net(X_temp)\n",
    "                #print(pred)\n",
    "                v_l += loss_func(pred, Y_temp)\n",
    "                total += Y.shape[0]\n",
    "                del X_temp\n",
    "                del Y_temp\n",
    "            v_l/=total\n",
    "\n",
    "            train_epoch_loss.append(t_l)\n",
    "            val_epoch_loss.append(v_l)\n",
    "            #print(\"bias:\", sum(pred))\n",
    "\n",
    "            print(\"train loss:\", train_epoch_loss[len(train_epoch_loss)-1])\n",
    "            print(\"val loss:\", val_epoch_loss[len(train_epoch_loss)-1])\n",
    "\n",
    "    return train_epoch_loss, val_epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4608\n",
      "avg initial loss: tensor(0.0527, device='cuda:0')\n",
      "5632\n",
      "base epoch # 0\n",
      "batchnum:  0\n",
      "tensor([ 0.1073,  0.0860, -0.1380,  0.1123, -0.1879, -0.1308, -0.1809,  0.0828,\n",
      "         0.1396, -0.1592], device='cuda:0', grad_fn=<ViewBackward>)\n",
      "tensor([1., 1., 1., 1., 0., 0., 0., 0., 1., 1.], device='cuda:0')\n",
      "tensor(0.6434, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "batchnum:  1000\n",
      "tensor([ 0.0266, -0.0011, -0.0376,  0.0307,  0.0415,  0.0212,  0.0521,  0.0531,\n",
      "         0.0797,  0.0480], device='cuda:0', grad_fn=<ViewBackward>)\n",
      "tensor([0., 1., 1., 0., 1., 1., 1., 0., 0., 0.], device='cuda:0')\n",
      "tensor(0.5162, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "batchnum:  2000\n",
      "tensor([0.0575, 0.0260, 0.0290, 0.0223, 0.0247, 0.0041, 0.0608, 0.0319, 0.0561,\n",
      "        0.0319], device='cuda:0', grad_fn=<ViewBackward>)\n",
      "tensor([0., 1., 1., 0., 1., 0., 1., 1., 0., 0.], device='cuda:0')\n",
      "tensor(0.4999, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "batchnum:  3000\n",
      "tensor([0.0703, 0.0230, 0.0107, 0.0115, 0.0325, 0.0178, 0.0207, 0.0230, 0.0129,\n",
      "        0.0230], device='cuda:0', grad_fn=<ViewBackward>)\n",
      "tensor([0., 1., 1., 0., 0., 1., 0., 1., 1., 0.], device='cuda:0')\n",
      "tensor(0.5071, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "batchnum:  4000\n",
      "tensor([ 0.0002,  0.0170,  0.0131,  0.0220,  0.0062,  0.0002, -0.0036, -0.0046,\n",
      "         0.0203,  0.0065], device='cuda:0', grad_fn=<ViewBackward>)\n",
      "tensor([1., 0., 0., 1., 0., 1., 1., 0., 1., 0.], device='cuda:0')\n",
      "tensor(0.5008, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "batchnum:  5000\n",
      "tensor([ 0.0236, -0.0041,  0.0296,  0.0081,  0.0105,  0.0174,  0.0200,  0.0247,\n",
      "         0.0074,  0.0003], device='cuda:0', grad_fn=<ViewBackward>)\n",
      "tensor([1., 0., 0., 0., 0., 1., 0., 0., 1., 0.], device='cuda:0')\n",
      "tensor(0.3049, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "batchnum:  6000\n",
      "tensor([ 0.0130,  0.0058,  0.0018,  0.0050,  0.0149, -0.0005,  0.0068,  0.0132,\n",
      "         0.0152,  0.0120], device='cuda:0', grad_fn=<ViewBackward>)\n",
      "tensor([0., 1., 1., 0., 0., 1., 0., 1., 0., 0.], device='cuda:0')\n",
      "tensor(0.4047, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "batchnum:  7000\n",
      "tensor([ 0.0062,  0.0002,  0.0002,  0.0164,  0.0182,  0.0027,  0.0121, -0.0014,\n",
      "         0.0168, -0.0003], device='cuda:0', grad_fn=<ViewBackward>)\n",
      "tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 0.], device='cuda:0')\n",
      "tensor(0.3971, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "train loss: tensor(0.0598, device='cuda:0')\n",
      "val loss: tensor(0.0447, device='cuda:0')\n",
      "18944\n",
      "base epoch # 1\n",
      "batchnum:  0\n",
      "tensor([ 0.0096,  0.0093,  0.0043,  0.0030,  0.0068, -0.0002,  0.0038, -0.0003,\n",
      "         0.0096,  0.0013], device='cuda:0', grad_fn=<ViewBackward>)\n",
      "tensor([1., 1., 1., 1., 0., 0., 0., 0., 1., 1.], device='cuda:0')\n",
      "tensor(0.5974, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "batchnum:  1000\n",
      "tensor([0.0040, 0.0035, 0.0024, 0.0026, 0.0022, 0.0005, 0.0032, 0.0039, 0.0034,\n",
      "        0.0054], device='cuda:0', grad_fn=<ViewBackward>)\n",
      "tensor([0., 1., 1., 0., 1., 1., 1., 0., 0., 0.], device='cuda:0')\n",
      "tensor(0.5007, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "batchnum:  2000\n",
      "tensor([0.0040, 0.0004, 0.0003, 0.0039, 0.0022, 0.0005, 0.0033, 0.0004, 0.0015,\n",
      "        0.0008], device='cuda:0', grad_fn=<ViewBackward>)\n",
      "tensor([0., 1., 1., 0., 1., 0., 1., 1., 0., 0.], device='cuda:0')\n",
      "tensor(0.5004, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "batchnum:  3000\n",
      "tensor([ 0.0009,  0.0023,  0.0009,  0.0014,  0.0028, -0.0003,  0.0019,  0.0004,\n",
      "         0.0023,  0.0022], device='cuda:0', grad_fn=<ViewBackward>)\n",
      "tensor([0., 1., 1., 0., 0., 1., 0., 1., 1., 0.], device='cuda:0')\n",
      "tensor(0.5004, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "batchnum:  4000\n",
      "tensor([ 2.9206e-06, -2.8420e-05,  1.6152e-03,  2.9206e-06,  1.4480e-03,\n",
      "         2.5342e-04,  7.7920e-04,  7.4160e-04,  2.7763e-04,  1.2276e-03],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>)\n",
      "tensor([1., 0., 0., 1., 0., 1., 1., 0., 1., 0.], device='cuda:0')\n",
      "tensor(0.5004, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "batchnum:  5000\n",
      "tensor([0.0019, 0.0009, 0.0005, 0.0003, 0.0011, 0.0002, 0.0003, 0.0023, 0.0004,\n",
      "        0.0009], device='cuda:0', grad_fn=<ViewBackward>)\n",
      "tensor([1., 0., 0., 0., 0., 1., 0., 0., 1., 0.], device='cuda:0')\n",
      "tensor(0.3004, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "batchnum:  6000\n",
      "tensor([ 0.0040,  0.0011,  0.0034,  0.0040,  0.0041,  0.0040,  0.0014,  0.0011,\n",
      "        -0.0005,  0.0039], device='cuda:0', grad_fn=<ViewBackward>)\n",
      "tensor([0., 1., 1., 0., 0., 1., 0., 1., 0., 0.], device='cuda:0')\n",
      "tensor(0.4008, device='cuda:0', grad_fn=<L1LossBackward>)\n",
      "batchnum:  7000\n",
      "tensor([0.0015, 0.0002, 0.0005, 0.0011, 0.0017, 0.0009, 0.0019, 0.0017, 0.0017,\n",
      "        0.0016], device='cuda:0', grad_fn=<ViewBackward>)\n",
      "tensor([0., 1., 0., 1., 1., 0., 0., 0., 1., 0.], device='cuda:0')\n",
      "tensor(0.4003, device='cuda:0', grad_fn=<L1LossBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-a8dfbc0556fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredNet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m.0001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory_allocated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_and_validate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredNet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mL1Loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-2fb9c4c1bfeb>\u001b[0m in \u001b[0;36mfit_and_validate\u001b[1;34m(net, loss_func, optimizer, train, val, n_epochs, batch_size)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[0mb\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[0mX_temp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m                 \u001b[0mY_temp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m                 \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_temp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m                 \u001b[1;31m#print(pred)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "predNet = priceNet().cuda()\n",
    "optimizer = optim.Adam(predNet.parameters(), lr = .0001)\n",
    "print(torch.cuda.memory_allocated(0))\n",
    "tl, vl = fit_and_validate(predNet, nn.L1Loss(), optimizer, train, val, 1000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_train_X = train_X.view(train_X[:,0,:,:].shape[0], -1)\n",
    "reg_val_X = val_X.view(val_X[:,0,:,:].shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train acc it = 1 : 0.5601457588231324\n",
      "val acc it = 1 : 0.5525264521701576\n",
      "\n",
      "train acc it = 2 : 0.5623861259194277\n",
      "val acc it = 2 : 0.5492334269056359\n",
      "\n",
      "train acc it = 5 : 0.5650988595721709\n",
      "val acc it = 5 : 0.5501511552580436\n",
      "\n",
      "train acc it = 10 : 0.5652608138200959\n",
      "val acc it = 10 : 0.5486396026776075\n",
      "\n",
      "train acc it = 20 : 0.5658276536878332\n",
      "val acc it = 20 : 0.5489635068019866\n"
     ]
    }
   ],
   "source": [
    "iters = [1, 2, 5, 10, 20]\n",
    "for i in iters:\n",
    "    reg = LogisticRegression(max_iter=i)\n",
    "    reg.fit(reg_train_X, train_Y)\n",
    "    print()\n",
    "    print(\"train acc it = \" + str(i),\":\", reg.score(reg_train_X, train_Y))\n",
    "    print(\"val acc it = \" + str(i), \":\", reg.score(reg_val_X, val_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4465018354567048\n"
     ]
    }
   ],
   "source": [
    "print(float(sum(val_Y))/val_Y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rfreg = RandomForestRegressor()\n",
    "rfreg.fit(reg_train_X, train_Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
